 **Shallow and Deep RVFL Neural Network Project**

This repository contains the implementation and analysis of Shallow and Deep Random Vector Functional Link (RVFL) Neural Networks, a project I worked on during my internship under the guidance of Prof. M. Tanveer and M. Sajid Sir at the Indian Institute of Technology (IIT) Indore.

This project holds a special place in my journey, as it helped me dive deep into neural networks beyond the traditional backpropagation-based approaches.  
Working with RVFL Networks—both shallow and deep—gave me insights into the power of randomized neural networks and closed-form solutions.

Through this work, I explored the trade-off between model simplicity and performance, learning that sometimes shallow networks work just as well as deep ones, while other times, depth is what unlocks the potential of the data.

About the Project
The project aims to compare the performance of shallow and deep RVFL networks across diverse datasets, focusing on how network depth affects accuracy and computational efficiency.
RVFL is a randomized feedforward neural network, where:
- Input-to-output direct links are introduced.
- Hidden layer weights are randomly initialized and not updated.
- Output weights are computed analytically.
Key aspects:
- Implementation of both Shallow and Deep RVFL Networks from scratch.
- Hyperparameter tuning for L (number of hidden layers) and C (regularization parameter).
- Evaluation on 8 diverse datasets from the UCI Machine Learning Repository.

Datasets Used
1. Car Evaluation
2. Optical Recognition of Handwritten Digits
3. Ozone Level Detection
4. Magic Gamma Telescope
5. Cardiotocography
6. Mushroom
7. Pageblocks
8. Pendigits

Each dataset presented unique challenges, allowing me to test the adaptability and efficiency of RVFL networks across simple and complex data patterns.

I learned:
- That randomized neural networks can perform surprisingly well.
- That sometimes deep models improve results—but not always.
- How choosing the right number of layers and regularization can make all the difference.
- Most importantly, I learned resilience—when things don’t work, you keep trying until they do.
This project has truly shaped my understanding of neural networks, and I’m proud to share it here.

Acknowledgement
This work was completed under the esteemed guidance of:
- Prof. M. Tanveer – IIT Indore (SERB Ramanujan Fellow and INSA Associate Fellow
IEEE CIS Distinguished Lecturer (2024-2026)
INSA Distinguished Lecture Fellow
Elected Board of Governors - APNNS (2023-2024)
Professor of Mathematics
Indian Institute of Technology Indore)
- M. Sajid Sir – IIT Indore (Department of Mathematics, Doctor of Philosophy)

Their support and mentorship were invaluable throughout this project, and I am deeply grateful for the opportunity to learn from them.
If you’ve made it this far—thank you for your time! 
This project represents months of learning, effort, and growth, and I hope it serves as a valuable resource for anyone exploring randomized neural networks.

-Zaina
Contact
- GitHub:[Zaina10](https://github.com/Zaina10)
- Email: zainaa1003@gmail.com
Feel free to use this code for learning purposes. If you build on it or find it helpful, a shoutout would mean the world to me!
